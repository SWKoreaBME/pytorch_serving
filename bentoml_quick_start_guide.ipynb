{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "name": "bentoml-quick-start-guide.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SWKoreaBME/pytorch_serving/blob/feature%2Fmlflow-tutorial/bentoml_quick_start_guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5aECpV5mbEM"
      },
      "source": [
        "# Getting Started with BentoML\n",
        "\n",
        "[BentoML](http://bentoml.ai) is an open-source framework for machine learning **model serving**, aiming to **bridge the gap between Data Science and DevOps**.\n",
        "\n",
        "Data Scientists can easily package their models trained with any ML framework using BentoMl and reproduce the model for serving in production. BentoML helps with managing packaged models in the BentoML format, and allows DevOps to deploy them as online API serving endpoints or offline batch inference jobs, on any cloud platform.\n",
        "\n",
        "This getting started guide demonstrates how to use BentoML to serve a sklearn modeld via a REST API server, and then containerize the model server for production deployment.\n",
        "\n",
        "![Impression](https://www.google-analytics.com/collect?v=1&tid=UA-112879361-3&cid=555&t=event&ec=guides&ea=bentoml-quick-start-guide&dt=bentoml-quick-start-guide)\n",
        "\n",
        "BentoML requires python 3.6 or above, install dependencies via `pip`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KePWRmImbEY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30b615c3-b5ce-4fcb-d434-8c7e4c9a5561"
      },
      "source": [
        "# Install PyPI packages required in this guide, including BentoML\n",
        "!pip install -q --pre bentoml  # install preview version of BentoML for this guide\n",
        "!pip install -q 'scikit-learn>=0.23.2' 'pandas>=1.1.1'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.8MB 11.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 28.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 8.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 41.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 153kB 40.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 38.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 40.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 378kB 40.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 9.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 37.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 42.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 7.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 40.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 153kB 42.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.4MB 38.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 8.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 7.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 552kB 37.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 9.3MB/s \n",
            "\u001b[?25h  Building wheel for sqlalchemy-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gunicorn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-json-logger (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: botocore 1.20.57 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 1.7MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BydT60qombEZ"
      },
      "source": [
        "Before starting, let's prepare a trained model for serving with BentoML. Train a classifier model on the [Iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAv_2caumbEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe9dc6d9-8243-4c27-80dd-04a0f6acd506"
      },
      "source": [
        "from sklearn import svm\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load training data\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Model Training\n",
        "clf = svm.SVC(gamma='scale')\n",
        "clf.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veOirs_3mbEc"
      },
      "source": [
        "## Create a Prediction Service with BentoML\n",
        "\n",
        "Model serving with BentoML comes after a model is trained. The first step is creating a\n",
        "prediction service class, which defines the models required and the inference APIs which\n",
        "contains the serving logic. Here is a minimal prediction service created for serving\n",
        "the iris classifier model trained above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FwIMhEHmbEc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1509c5b3-b96d-488d-d7b0-b8c432c2ddba"
      },
      "source": [
        "%%writefile iris_classifier.py\n",
        "import pandas as pd\n",
        "\n",
        "from bentoml import env, artifacts, api, BentoService\n",
        "from bentoml.adapters import DataframeInput\n",
        "from bentoml.frameworks.sklearn import SklearnModelArtifact\n",
        "\n",
        "@env(infer_pip_packages=True)\n",
        "@artifacts([SklearnModelArtifact('model')])\n",
        "class IrisClassifier(BentoService):\n",
        "    \"\"\"\n",
        "    A minimum prediction service exposing a Scikit-learn model\n",
        "    \"\"\"\n",
        "\n",
        "    @api(input=DataframeInput(), batch=True)\n",
        "    def predict(self, df: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        An inference API named `predict` with Dataframe input adapter, which codifies\n",
        "        how HTTP requests or CSV files are converted to a pandas Dataframe object as the\n",
        "        inference API function input\n",
        "        \"\"\"\n",
        "        return self.artifacts.model.predict(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing iris_classifier.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxUbnStMmbEd"
      },
      "source": [
        "This code defines a prediction service that packages a scikit-learn model and provides\n",
        "an inference API that expects a `pandas.Dataframe` object as its input. BentoML also supports other API input \n",
        "data types including `JsonInput`, `ImageInput`, `FileInput` and \n",
        "[more](https://docs.bentoml.org/en/latest/api/adapters.html).\n",
        "\n",
        "\n",
        "In BentoML, **all inference APIs are suppose to accept a list of inputs and return a \n",
        "list of results**. In the case of `DataframeInput`, each row of the dataframe is mapping\n",
        "to one prediction request received from the client. BentoML will convert HTTP JSON \n",
        "requests into :code:`pandas.DataFrame` object before passing it to the user-defined \n",
        "inference API function.\n",
        " \n",
        "This design allows BentoML to group API requests into small batches while serving online\n",
        "traffic. Comparing to a regular flask or FastAPI based model server, this can increases\n",
        "the overall throughput of the API server by 10-100x depending on the workload.\n",
        "\n",
        "The following code packages the trained model with the prediction service class\n",
        "`IrisClassifier` defined above, and then saves the IrisClassifier instance to disk \n",
        "in the BentoML format for distribution and deployment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rs-8_Z57mbEe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad0b0a6-e393-4be9-c504-0232360bac77"
      },
      "source": [
        "# import the IrisClassifier class defined above\n",
        "from iris_classifier import IrisClassifier\n",
        "\n",
        "# Create a iris classifier service instance\n",
        "iris_classifier_service = IrisClassifier()\n",
        "\n",
        "# Pack the newly trained model artifact\n",
        "iris_classifier_service.pack('model', clf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-04-26 09:58:42,897] WARNING - pip package requirement pandas already exist\n",
            "[2021-04-26 09:58:42,900] WARNING - pip package requirement scikit-learn already exist\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<iris_classifier.IrisClassifier at 0x7f3ee5a031d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_RJbr9HmbEf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a0169f14-b86a-4d43-e930-e96493b8fb85"
      },
      "source": [
        "# Prepare input data for testing the prediction service\n",
        "import pandas as pd\n",
        "test_input_df = pd.DataFrame(X).sample(n=5)\n",
        "test_input_df.to_csv(\"./test_input.csv\", index=False)\n",
        "test_input_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>6.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>4.5</td>\n",
              "      <td>1.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>5.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>5.7</td>\n",
              "      <td>2.8</td>\n",
              "      <td>4.1</td>\n",
              "      <td>1.3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0    1    2    3\n",
              "85  6.0  3.4  4.5  1.6\n",
              "28  5.2  3.4  1.4  0.2\n",
              "3   4.6  3.1  1.5  0.2\n",
              "46  5.1  3.8  1.6  0.2\n",
              "99  5.7  2.8  4.1  1.3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g0lIt3SmbEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25065ddc-11ca-4dc5-9640-f16c27dc50be"
      },
      "source": [
        "# Test the service's inference API python interface\n",
        "iris_classifier_service.predict(test_input_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKdzhNllmbEg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "608a230d-5c0d-4c1b-85ae-17f1392ea60d"
      },
      "source": [
        "# Start a dev model server to test out everything\n",
        "iris_classifier_service.start_dev_server()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-04-26 10:01:58,641] INFO - BentoService bundle 'IrisClassifier:20210426100157_EB2BA9' created at: /tmp/tmpwo05cka4\n",
            "[2021-04-26 10:01:58,653] INFO - ======= starting dev server on port: 5000 =======\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46yu1cAWmbEh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93d8cd9f-8a23-4715-b0b4-5bafed59d0cd"
      },
      "source": [
        "import requests\n",
        "response = requests.post(\n",
        "    \"http://127.0.0.1:5000/predict\",\n",
        "    json=test_input_df.values.tolist()\n",
        ")\n",
        "print(response.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 0, 0, 0, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wycUn0WWmbEh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02a0e541-1436-4920-9f3a-7e10ce5b1ad5"
      },
      "source": [
        "# Stop the dev model server\n",
        "iris_classifier_service.stop_dev_server()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-04-26 10:03:10,807] INFO - Dev server has stopped.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y-vT_k8mbEi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a326401-6b4a-4a49-f384-f626e2599b1b"
      },
      "source": [
        "# Save the prediction service to disk for deployment\n",
        "saved_path = iris_classifier_service.save()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2021-04-26 10:03:36,217] INFO - BentoService bundle 'IrisClassifier:20210426100157_EB2BA9' saved to: /root/bentoml/repository/IrisClassifier/20210426100157_EB2BA9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wExbU32ambEi"
      },
      "source": [
        "BentoML stores all packaged model files under the\n",
        "`~/bentoml/{service_name}/{service_version}` directory by default.\n",
        "The BentoML file format contains all the code, files, and configs required to \n",
        "deploy the model for serving.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f5FUfLumbEj"
      },
      "source": [
        "## REST API Model Serving\n",
        "\n",
        "\n",
        "\n",
        "To start a REST API model server with the `IrisClassifier` saved above, use \n",
        "the `bentoml serve` command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "8juozJhvmbEj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9bfdda5-4997-46c8-c7c7-3b0b25cdf84f"
      },
      "source": [
        "!bentoml serve IrisClassifier:latest"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "[2021-04-26 10:04:33,383] INFO - Getting latest version IrisClassifier:20210426100157_EB2BA9\n",
            "[2021-04-26 10:04:33,402] INFO - Starting BentoML API proxy in development mode..\n",
            "[2021-04-26 10:04:33,405] INFO - Starting BentoML API server in development mode..\n",
            "[2021-04-26 10:04:33,545] INFO - Your system nofile limit is 1048576, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.\n",
            "======== Running on http://0.0.0.0:5000 ========\n",
            "(Press CTRL+C to quit)\n",
            " * Serving Flask app \"IrisClassifier\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n",
            " * Running on http://127.0.0.1:47639/ (Press CTRL+C to quit)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GZdW_axmbEj"
      },
      "source": [
        "If you are running this notebook from Google Colab, you can start the dev server with `--run-with-ngrok` option, to gain access to the API endpoint via a public endpoint managed by [ngrok](https://ngrok.com/): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1rKuUAG-mbEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4df66f62-0044-4f67-9b9b-517f7d68515b"
      },
      "source": [
        "!bentoml serve IrisClassifier:latest --run-with-ngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "[2021-04-26 10:05:33,908] INFO - Getting latest version IrisClassifier:20210426100157_EB2BA9\n",
            "[2021-04-26 10:05:33,925] INFO - Starting BentoML API proxy in development mode..\n",
            "[2021-04-26 10:05:33,927] INFO - Starting BentoML API server in development mode..\n",
            "[2021-04-26 10:05:34,056] INFO - Your system nofile limit is 1048576, which means each instance of microbatch service is able to hold this number of connections at same time. You can increase the number of file descriptors for the server process, or launch more microbatch instances to accept more concurrent connection.\n",
            "======== Running on http://0.0.0.0:5000 ========\n",
            "(Press CTRL+C to quit)\n",
            " * Serving Flask app \"IrisClassifier\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n",
            " * Running on http://127.0.0.1:50113/ (Press CTRL+C to quit)\n",
            "[2021-04-26 10:05:37,554] INFO -  * Running on http://e6fbd95bb181.ngrok.io\n",
            "[2021-04-26 10:05:37,555] INFO -  * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WsWHAphmbEk"
      },
      "source": [
        "The `IrisClassifier` model is now served at `localhost:5000`. Use `curl` command to send\n",
        "a prediction request:\n",
        "\n",
        "```bash\n",
        "curl -i \\\n",
        "--header \"Content-Type: application/json\" \\\n",
        "--request POST \\\n",
        "--data '[[5.1, 3.5, 1.4, 0.2]]' \\\n",
        "localhost:5000/predict\n",
        "```\n",
        "\n",
        "Or with `python` and [request library](https://requests.readthedocs.io/):\n",
        "```python\n",
        "import requests\n",
        "response = requests.post(\"http://127.0.0.1:5000/predict\", json=[[5.1, 3.5, 1.4, 0.2]])\n",
        "print(response.text)\n",
        "```\n",
        "\n",
        "Note that BentoML API server automatically converts the Dataframe JSON format into a\n",
        "`pandas.DataFrame` object before sending it to the user-defined inference API function.\n",
        "\n",
        "The BentoML API server also provides a simple web UI dashboard.\n",
        "Go to http://localhost:5000 in the browser and use the Web UI to send\n",
        "prediction request:\n",
        "\n",
        "![BentoML API Server Web UI Screenshot](https://raw.githubusercontent.com/bentoml/BentoML/master/guides/quick-start/bento-api-server-web-ui.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTGm_ZcVmbEl"
      },
      "source": [
        "## Containerize model server with Docker\n",
        "\n",
        "\n",
        "\n",
        "One common way of distributing this model API server for production deployment, is via\n",
        "Docker containers. And BentoML provides a convenient way to do that.\n",
        "\n",
        "Note that `docker` is __not available in Google Colab__. You will need to download and run this notebook locally to try out this containerization with docker feature.\n",
        "\n",
        "If you already have docker configured, simply run the follow command to product a \n",
        "docker container serving the `IrisClassifier` prediction service created above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dafRd6mZmbEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fa4cd48-c08d-4c3a-973b-7d82941f2de2"
      },
      "source": [
        "!bentoml containerize IrisClassifier:latest -t iris-classifier:v1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "[2021-04-26 10:08:44,711] INFO - Getting latest version IrisClassifier:20210426100157_EB2BA9\n",
            "\u001b[39mFound Bento: /root/bentoml/repository/IrisClassifier/20210426100157_EB2BA9\u001b[0m\n",
            "Containerizing IrisClassifier:20210426100157_EB2BA9 with local YataiService and docker daemon from local environment-\b/\b|\b\\\b-\b/\b|[2021-04-26 10:08:45,408] ERROR - RPC ERROR ContainerizeBento: Docker is required for this deployment. Please visit www.docker.com for instructions\n",
            "Error: \u001b[31mbentoml-cli containerize failed: Failed to containerize IrisClassifier:20210426100157_EB2BA9 - INTERNAL:Docker is required for this deployment. Please visit www.docker.com for instructions\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNguHco8mbEl"
      },
      "source": [
        "Start a container with the docker image built in the previous step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aetMDYsKmbEm",
        "outputId": "a92508c1-0e17-45a7-9e57-e174522afe32"
      },
      "source": [
        "!docker run -p 5000:5000 iris-classifier:v1 --workers=2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-03-19 09:37:42,276] INFO - Starting BentoML API server in production mode..\n",
            "[2021-03-19 09:37:42 +0000] [1] [INFO] Starting gunicorn 20.0.4\n",
            "[2021-03-19 09:37:42 +0000] [1] [INFO] Listening at: http://0.0.0.0:5000 (1)\n",
            "[2021-03-19 09:37:42 +0000] [1] [INFO] Using worker: sync\n",
            "[2021-03-19 09:37:42 +0000] [12] [INFO] Booting worker with pid: 12\n",
            "[2021-03-19 09:37:42 +0000] [13] [INFO] Booting worker with pid: 13\n",
            "[2021-03-19 09:37:42,700] WARNING - Saved BentoService Python version mismatch: loading BentoService bundle created with Python version 3.7.8, but current environment version is 3.7.6.\n",
            "[2021-03-19 09:37:42,771] WARNING - Saved BentoService Python version mismatch: loading BentoService bundle created with Python version 3.7.8, but current environment version is 3.7.6.\n",
            "[2021-03-19 09:38:35,545] INFO - {'service_name': 'IrisClassifier', 'service_version': '20210319023551_84AAF6', 'api': 'predict', 'task': {'data': '[[5.5, 2.3, 4.0, 1.3], [5.2, 4.1, 1.5, 0.1], [4.4, 2.9, 1.4, 0.2], [5.1, 3.8, 1.9, 0.4], [6.7, 3.3, 5.7, 2.1]]', 'task_id': '3c12e3f8-f7e0-47ed-a055-ed0e62623e6e', 'batch': 5, 'http_headers': (('Host', 'localhost:5000'), ('User-Agent', 'curl/7.71.1'), ('Accept', '*/*'), ('Content-Type', 'application/json'), ('Content-Length', '110'))}, 'result': {'data': '[1, 0, 0, 0, 2]', 'http_status': 200, 'http_headers': (('Content-Type', 'application/json'),)}, 'request_id': '3c12e3f8-f7e0-47ed-a055-ed0e62623e6e'}\n",
            "^C\n",
            "[2021-03-19 09:38:49 +0000] [1] [INFO] Handling signal: int\n",
            "[2021-03-19 09:38:49 +0000] [12] [INFO] Worker exiting (pid: 12)\n",
            "[2021-03-19 09:38:49 +0000] [13] [INFO] Worker exiting (pid: 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nngZILcNmbEm"
      },
      "source": [
        "This made it possible to deploy BentoML bundled ML models with platforms such as\n",
        "[Kubeflow](https://www.kubeflow.org/docs/components/serving/bentoml/),\n",
        "[Knative](https://knative.dev/community/samples/serving/machinelearning-python-bentoml/),\n",
        "[Kubernetes](https://docs.bentoml.org/en/latest/deployment/kubernetes.html), which\n",
        "provides advanced model deployment features such as auto-scaling, A/B testing,\n",
        "scale-to-zero, canary rollout and multi-armed bandit.\n",
        "\n",
        "\n",
        "## Load saved BentoService\n",
        "\n",
        "`bentoml.load` is the API for loading a BentoML packaged model in python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5q3MvV0mbEn",
        "outputId": "c375fd87-9e6a-428c-ef0b-abeeeacce46c"
      },
      "source": [
        "import bentoml\n",
        "import pandas as pd\n",
        "\n",
        "bento_svc = bentoml.load(saved_path)\n",
        "\n",
        "# Test loaded bentoml service:\n",
        "bento_svc.predict(test_input_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-03-19 02:38:54,032] WARNING - Module `iris_classifier` already loaded, using existing imported module.\n",
            "[2021-03-19 02:38:54,071] WARNING - pip package requirement pandas already exist\n",
            "[2021-03-19 02:38:54,072] WARNING - pip package requirement scikit-learn already exist\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "memmap([1, 0, 0, 0, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mldU2X0bmbEn"
      },
      "source": [
        "The BentoML format is pip-installable and can be directly distributed as a\n",
        "PyPI package for using in python applications:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLH3-kTRmbEn",
        "outputId": "b8a5d2c0-040b-4439-b490-e1d12e8d1425"
      },
      "source": [
        "!pip install -q {saved_path}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m  WARNING: Built wheel for IrisClassifier is invalid: Metadata 1.2 mandates PEP 440 version, but '20210319023551-84AAF6' is not\u001b[0m\n",
            "\u001b[33m  DEPRECATION: IrisClassifier was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR0lFNP6mbEo",
        "outputId": "d1a7ca5e-b408-468d-e834-0b99de037967"
      },
      "source": [
        "# The BentoService class name will become packaged name\n",
        "import IrisClassifier\n",
        "\n",
        "installed_svc = IrisClassifier.load()\n",
        "installed_svc.predict(test_input_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "memmap([1, 0, 0, 0, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLKNRUPqmbEo"
      },
      "source": [
        "This also allow users to upload their BentoService to pypi.org as public python package\n",
        "or to their organization's private PyPi index to share with other developers.\n",
        "\n",
        "`cd {saved_path} & python setup.py sdist upload`\n",
        "\n",
        "*You will have to configure \".pypirc\" file before uploading to pypi index.\n",
        "    You can find more information about distributing python package at:\n",
        "    https://docs.python.org/3.7/distributing/index.html#distributing-index*\n",
        "\n",
        "\n",
        "# Launch inference job from CLI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eOzfiiSmbEp"
      },
      "source": [
        "BentoML cli supports loading and running a packaged model from CLI. With the `DataframeInput` adapter, the CLI command supports reading input Dataframe data from CLI argument or local `csv` or `json` files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1x-IPspmbEp",
        "outputId": "4c6605db-c3b0-491d-c22f-1c99198b6fca"
      },
      "source": [
        "!bentoml run IrisClassifier:latest predict --input '{test_input_df.to_json()}' --quiet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 0, 0, 0, 2]\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMa4GJ6WmbEq",
        "outputId": "8781a08b-679e-4ab5-a0b1-43af3b1e56e3"
      },
      "source": [
        "!bentoml run IrisClassifier:latest predict \\\n",
        "    --input-file \"./test_input.csv\" --format \"csv\" --quiet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 0, 0, 0, 2]\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxIoMg_-mbEs",
        "outputId": "69b33463-1245-4cca-a624-d1b59ff887a4"
      },
      "source": [
        "# run inference with the docker image built above\n",
        "!docker run -v $(PWD):/tmp iris-classifier:v1 \\\n",
        "        bentoml run /bento predict --input-file \"/tmp/test_input.csv\" --format \"csv\" --quiet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 0, 0, 0, 2]\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q835bThimbEs"
      },
      "source": [
        "# Deployment Options\n",
        "\n",
        "Check out the [BentoML deployment guide](https://docs.bentoml.org/en/latest/deployment/index.html)\n",
        "to better understand which deployment option is best suited for your use case.\n",
        "\n",
        "* One-click deployment with BentoML:\n",
        "  - [AWS Lambda](https://docs.bentoml.org/en/latest/deployment/aws_lambda.html)\n",
        "  - [AWS SageMaker](https://docs.bentoml.org/en/latest/deployment/aws_sagemaker.html)\n",
        "  - [AWS EC2](https://docs.bentoml.org/en/latest/deployment/aws_ec2.html)\n",
        "  - [Azure Functions](https://docs.bentoml.org/en/latest/deployment/azure_functions.html)\n",
        "\n",
        "* Deploy with open-source platforms:\n",
        "  - [Docker](https://docs.bentoml.org/en/latest/deployment/docker.html)\n",
        "  - [Kubernetes](https://docs.bentoml.org/en/latest/deployment/kubernetes.html)\n",
        "  - [Knative](https://docs.bentoml.org/en/latest/deployment/knative.html)\n",
        "  - [Kubeflow](https://docs.bentoml.org/en/latest/deployment/kubeflow.html)\n",
        "  - [KFServing](https://docs.bentoml.org/en/latest/deployment/kfserving.html)\n",
        "  - [Clipper](https://docs.bentoml.org/en/latest/deployment/clipper.html)\n",
        "\n",
        "* Manual cloud deployment guides:\n",
        "  - [AWS ECS](https://docs.bentoml.org/en/latest/deployment/aws_ecs.html)\n",
        "  - [Google Cloud Run](https://docs.bentoml.org/en/latest/deployment/google_cloud_run.html)\n",
        "  - [Azure container instance](https://docs.bentoml.org/en/latest/deployment/azure_container_instance.html)\n",
        "  - [Heroku](https://docs.bentoml.org/en/latest/deployment/heroku.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaFJKGaXmbEt"
      },
      "source": [
        "# Summary\n",
        "\n",
        "This is what it looks like when using BentoML to serve and deploy a model in the cloud. BentoML also supports [many other Machine Learning frameworks](https://docs.bentoml.org/en/latest/examples.html) besides Scikit-learn. The [BentoML core concepts](https://docs.bentoml.org/en/latest/concepts.html) doc is recommended for anyone looking to get a deeper understanding of BentoML.\n",
        "\n",
        "Join the [BentoML Slack](https://join.slack.com/t/bentoml/shared_invite/enQtNjcyMTY3MjE4NTgzLTU3ZDc1MWM5MzQxMWQxMzJiNTc1MTJmMzYzMTYwMjQ0OGEwNDFmZDkzYWQxNzgxYWNhNjAxZjk4MzI4OGY1Yjg) to follow the latest development updates and roadmap discussions."
      ]
    }
  ]
}